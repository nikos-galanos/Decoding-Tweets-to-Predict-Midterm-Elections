{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m31nlaf5w_bW",
    "outputId": "ce151923-51c7-4a81-d784-03a3acb7bacd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0px5RDmvy_3c",
    "outputId": "78983247-2fe4-4fed-c336-ad53fc565bf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "5gJwhPJ0sxmm"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# importing the pipeline module\n",
    "from transformers import pipeline\n",
    "from google.colab import files\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from wordcloud import STOPWORDS\n",
    "import requests\n",
    "import re, string\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W6OCQyOiuBJ4"
   },
   "outputs": [],
   "source": [
    "# Reading the tweets data\n",
    "df_tweets = pd.read_excel('tweets.xlsx')\n",
    "df_replies = pd.read_excel('replies.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dhLcpXRJtIh6"
   },
   "outputs": [],
   "source": [
    "# Downloading the sentiment analysis model\n",
    "SentimentClassifier = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iWg3TQXntMft"
   },
   "outputs": [],
   "source": [
    "# Defining a function to call for the whole dataframe\n",
    "def FunctionBERTSentiment(inpText):\n",
    "  return(SentimentClassifier(inpText)[0]['label'])\n",
    "\n",
    "# Defining a function to call for the whole dataframe\n",
    "def FunctionBERTSentimentScore(inpText):\n",
    "  return(SentimentClassifier(inpText)[0]['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86flpXHot7RH"
   },
   "source": [
    "Senator's Tweets Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MUQsNL6Jt_mN"
   },
   "outputs": [],
   "source": [
    "df = df_tweets\n",
    "\n",
    "##CUSTOM DEFINED FUNCTIONS TO CLEAN THE TWEETS\n",
    "\n",
    "#Clean emojis from text\n",
    "def strip_emoji(text):\n",
    "    #return re.sub(emoji.get_emoji_regexp(), r\"\", text) #remove emoji\n",
    "    emojis = sorted(emoji.EMOJI_DATA, key=len, reverse=True)\n",
    "    pattern = u'(' + u'|'.join(re.escape(u) for u in emojis) + u')'\n",
    "    #return re.compile(pattern)\n",
    "    return re.sub(re.compile(pattern), r\"\", text) #remove emoji\n",
    "\n",
    "#Remove punctuations, links, mentions and \\r\\n new line characters\n",
    "def strip_all_entities(text): \n",
    "    text = text.replace('\\r', '').replace('\\n', ' ').replace('\\n', ' ').lower() #remove \\n and \\r and lowercase\n",
    "    text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", text) #remove links and mentions\n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r'', text) #remove non utf8/ascii characters such as '\\x9a\\x91\\x97\\x9a\\x97'\n",
    "    banned_list= string.punctuation + 'Ã'+'±'+'ã'+'¼'+'â'+'»'+'§'\n",
    "    table = str.maketrans('', '', banned_list)\n",
    "    text = text.translate(table)\n",
    "    return text\n",
    "\n",
    "#clean hashtags at the end of the sentence, and keep those in the middle of the sentence by removing just the # symbol\n",
    "def clean_hashtags(tweet):\n",
    "    new_tweet = \" \".join(word.strip() for word in re.split('#(?!(?:hashtag)\\b)[\\w-]+(?=(?:\\s+#[\\w-]+)*\\s*$)', tweet)) #remove last hashtags\n",
    "    new_tweet2 = \" \".join(word.strip() for word in re.split('#|_', new_tweet)) #remove hashtags symbol from words in the middle of the sentence\n",
    "    return new_tweet2\n",
    "\n",
    "#Filter special characters such as & and $ present in some words\n",
    "def filter_chars(a):\n",
    "    sent = []\n",
    "    for word in a.split(' '):\n",
    "        if ('$' in word) | ('&' in word):\n",
    "            sent.append('')\n",
    "        else:\n",
    "            sent.append(word)\n",
    "    return ' '.join(sent)\n",
    "\n",
    "def remove_mult_spaces(text): # remove multiple spaces\n",
    "    return re.sub(\"\\s\\s+\" , \" \", text)\n",
    "\n",
    "\n",
    "texts_new = []\n",
    "for t in df.text:\n",
    "    texts_new.append(remove_mult_spaces(filter_chars(clean_hashtags(strip_all_entities(strip_emoji(t))))))\n",
    "\n",
    "df['text_clean'] = texts_new\n",
    "\n",
    "text_len = []\n",
    "for text in df.text_clean:\n",
    "    tweet_len = len(text.split())\n",
    "    text_len.append(tweet_len)\n",
    "\n",
    "df['text_len'] = text_len\n",
    "\n",
    "df = df[df['text_len'] > 4]\n",
    "df_tweets = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FY5CG8bCt1q1"
   },
   "source": [
    "Analysis on Senator's Tweets Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H7UOZB24tNFh"
   },
   "outputs": [],
   "source": [
    "# Calling BERT based sentiment score function for every tweet\n",
    "df_tweets['Sentiment']=df_tweets['text_clean'].apply(FunctionBERTSentiment)\n",
    "df_tweets['Sentiment Score']=df_tweets['text_clean'].apply(FunctionBERTSentimentScore)\n",
    "#df_tweets.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7thXYM9_zUNs"
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', None)\n",
    "pd.set_option('display.width', 30000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ml2AucYzMJo"
   },
   "outputs": [],
   "source": [
    "#df_tweets[['text_clean','Sentiment','Sentiment Score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Q76h1QPtUYE"
   },
   "outputs": [],
   "source": [
    "# Visualizing the overall sentiment distribution\n",
    "fig, subPlot =plt.subplots(nrows=1, ncols=2, figsize=(10,4))\n",
    "fig.suptitle(\"Sentiment analysis of Twitter Tweets\")\n",
    " \n",
    "# Grouping the data\n",
    "GroupedData=df_tweets.groupby('Sentiment').size()\n",
    " \n",
    "# Creating the charts\n",
    "GroupedData.plot(kind='bar', ax=subPlot[0], color=['crimson', 'lightblue'])\n",
    "GroupedData.plot(kind='pie', ax=subPlot[1], colors=['crimson', 'lightblue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7KV_bobEtVF4"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Let's count the number of tweets by sentiments\n",
    "sentiment_counts = df_tweets.groupby(['Sentiment']).size()\n",
    "print(sentiment_counts)\n",
    "\n",
    "# Let's visualize the sentiments\n",
    "fig = plt.figure(figsize=(6,6), dpi=100)\n",
    "ax = plt.subplot(111)\n",
    "sentiment_counts.plot.pie(ax=ax, autopct='%1.1f%%', startangle=270, fontsize=12, label=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YFheO8eWtdkP"
   },
   "outputs": [],
   "source": [
    "# Wordcloud with positive tweets\n",
    "positive_tweets = df_tweets['text_clean'][df_tweets[\"Sentiment\"] == 'POSITIVE']\n",
    "stop_words = [\"https\", \"co\", \"RT\"] + list(STOPWORDS)\n",
    "positive_wordcloud = WordCloud(max_font_size=50, max_words=50, background_color=\"white\", stopwords = stop_words).generate(str(positive_tweets))\n",
    "plt.figure()\n",
    "plt.title(\"Positive Tweets - Wordcloud\")\n",
    "plt.imshow(positive_wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Wordcloud with negative tweets\n",
    "negative_tweets = df_tweets['text_clean'][df_tweets[\"Sentiment\"] == 'NEGATIVE']\n",
    "stop_words = [\"https\", \"co\", \"RT\"] + list(STOPWORDS)\n",
    "negative_wordcloud = WordCloud(max_font_size=50, max_words=50, background_color=\"white\", stopwords = stop_words).generate(str(negative_tweets))\n",
    "plt.figure()\n",
    "plt.title(\"Negative Tweets - Wordcloud\")\n",
    "plt.imshow(negative_wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mYJ--OCf1J4g"
   },
   "outputs": [],
   "source": [
    "df_tweets.to_excel(\"final_senators_tweets_sentiments.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "07DLUhQA1Jpp"
   },
   "outputs": [],
   "source": [
    "files.download(\"final_senators_tweets_sentiments.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sL-vYXrLvW_d"
   },
   "source": [
    "Replies Tweets Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DslmaCOsvaW1"
   },
   "outputs": [],
   "source": [
    "df = df_replies\n",
    "\n",
    "##CUSTOM DEFINED FUNCTIONS TO CLEAN THE TWEETS\n",
    "\n",
    "#Clean emojis from text\n",
    "def strip_emoji(text):\n",
    "    #return re.sub(emoji.get_emoji_regexp(), r\"\", text) #remove emoji\n",
    "    emojis = sorted(emoji.EMOJI_DATA, key=len, reverse=True)\n",
    "    pattern = u'(' + u'|'.join(re.escape(u) for u in emojis) + u')'\n",
    "    #return re.compile(pattern)\n",
    "    return re.sub(re.compile(pattern), r\"\", text) #remove emoji\n",
    "\n",
    "#Remove punctuations, links, mentions and \\r\\n new line characters\n",
    "def strip_all_entities(text): \n",
    "    text = text.replace('\\r', '').replace('\\n', ' ').replace('\\n', ' ').lower() #remove \\n and \\r and lowercase\n",
    "    text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", text) #remove links and mentions\n",
    "    text = re.sub(r'[^\\x00-\\x7f]',r'', text) #remove non utf8/ascii characters such as '\\x9a\\x91\\x97\\x9a\\x97'\n",
    "    banned_list= string.punctuation + 'Ã'+'±'+'ã'+'¼'+'â'+'»'+'§'\n",
    "    table = str.maketrans('', '', banned_list)\n",
    "    text = text.translate(table)\n",
    "    return text\n",
    "\n",
    "#clean hashtags at the end of the sentence, and keep those in the middle of the sentence by removing just the # symbol\n",
    "def clean_hashtags(tweet):\n",
    "    new_tweet = \" \".join(word.strip() for word in re.split('#(?!(?:hashtag)\\b)[\\w-]+(?=(?:\\s+#[\\w-]+)*\\s*$)', tweet)) #remove last hashtags\n",
    "    new_tweet2 = \" \".join(word.strip() for word in re.split('#|_', new_tweet)) #remove hashtags symbol from words in the middle of the sentence\n",
    "    return new_tweet2\n",
    "\n",
    "#Filter special characters such as & and $ present in some words\n",
    "def filter_chars(a):\n",
    "    sent = []\n",
    "    for word in a.split(' '):\n",
    "        if ('$' in word) | ('&' in word):\n",
    "            sent.append('')\n",
    "        else:\n",
    "            sent.append(word)\n",
    "    return ' '.join(sent)\n",
    "\n",
    "def remove_mult_spaces(text): # remove multiple spaces\n",
    "    return re.sub(\"\\s\\s+\" , \" \", text)\n",
    "\n",
    "\n",
    "texts_new = []\n",
    "for t in df.text:\n",
    "    texts_new.append(remove_mult_spaces(filter_chars(clean_hashtags(strip_all_entities(strip_emoji(t))))))\n",
    "\n",
    "df['text_clean'] = texts_new\n",
    "\n",
    "text_len = []\n",
    "for text in df.text_clean:\n",
    "    tweet_len = len(text.split())\n",
    "    text_len.append(tweet_len)\n",
    "\n",
    "df['text_len'] = text_len\n",
    "\n",
    "df = df[df['text_len'] > 4]\n",
    "df_replies = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8jEKr7TDtxzK"
   },
   "source": [
    "Analysis on Replies Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eqctdIb3tz3U"
   },
   "outputs": [],
   "source": [
    "# Calling BERT based sentiment score function for every tweet\n",
    "df_replies['Sentiment']=df_replies['text_clean'].apply(FunctionBERTSentiment)\n",
    "df_replies['Sentiment Score']=df_replies['text_clean'].apply(FunctionBERTSentimentScore)\n",
    "#df_replies.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJimQuxOzjwe"
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', None)\n",
    "pd.set_option('display.width', 30000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lcVQvTo5zjos"
   },
   "outputs": [],
   "source": [
    "#df_replies[['text_clean','Sentiment','Sentiment Score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wPeauke3uKvR"
   },
   "outputs": [],
   "source": [
    "# Visualizing the overall sentiment distribution\n",
    "fig, subPlot =plt.subplots(nrows=1, ncols=2, figsize=(10,4))\n",
    "fig.suptitle(\"Sentiment analysis of Twitter Tweets\")\n",
    " \n",
    "# Grouping the data\n",
    "GroupedData=df_replies.groupby('Sentiment').size()\n",
    " \n",
    "# Creating the charts\n",
    "GroupedData.plot(kind='bar', ax=subPlot[0], color=['crimson', 'lightblue'])\n",
    "GroupedData.plot(kind='pie', ax=subPlot[1], colors=['crimson', 'lightblue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ywmDr3quuLJm"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Let's count the number of tweets by sentiments\n",
    "sentiment_counts = df_replies.groupby(['Sentiment']).size()\n",
    "print(sentiment_counts)\n",
    "\n",
    "# Let's visualize the sentiments\n",
    "fig = plt.figure(figsize=(6,6), dpi=100)\n",
    "ax = plt.subplot(111)\n",
    "sentiment_counts.plot.pie(ax=ax, autopct='%1.1f%%', startangle=270, fontsize=12, label=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNYgFQ-OuLSI"
   },
   "outputs": [],
   "source": [
    "# Wordcloud with positive tweets\n",
    "positive_tweets = df_replies['text_clean'][df_replies[\"Sentiment\"] == 'POSITIVE']\n",
    "stop_words = [\"https\", \"co\", \"RT\"] + list(STOPWORDS)\n",
    "positive_wordcloud = WordCloud(max_font_size=50, max_words=50, background_color=\"white\", stopwords = stop_words).generate(str(positive_tweets))\n",
    "plt.figure()\n",
    "plt.title(\"Positive Tweets - Wordcloud\")\n",
    "plt.imshow(positive_wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Wordcloud with negative tweets\n",
    "negative_tweets = df_replies['text_clean'][df_replies[\"Sentiment\"] == 'NEGATIVE']\n",
    "stop_words = [\"https\", \"co\", \"RT\"] + list(STOPWORDS)\n",
    "negative_wordcloud = WordCloud(max_font_size=50, max_words=50, background_color=\"white\", stopwords = stop_words).generate(str(negative_tweets))\n",
    "plt.figure()\n",
    "plt.title(\"Negative Tweets - Wordcloud\")\n",
    "plt.imshow(negative_wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MVXV4g0X00L8"
   },
   "outputs": [],
   "source": [
    "df_replies.to_excel(\"final_replies_tweets_sentiments.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GtGXv5wr2_Ab"
   },
   "outputs": [],
   "source": [
    "files.download(\"final_replies_tweets_sentiments.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zsFDHWMV30N7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
